#!/usr/bin/env bash

pbs_user='mpotgieter1'  # user that apears in qstat 
pbs_queue_limit=1      # throttle queues at this number

# Configuration for the BLAST pipeline
bp_log_file='/mnt/lustre/users/mpotgieter1/andrew_stool_out/jobs.txt'
bp_pbs_P='CBBI0825'  # Ptoject code goes here
output_folder='/mnt/lustre/users/mpotgieter1/andrew_stool_out/andrew_PAM30'
bp_input_fasta='/mnt/lustre/users/mpotgieter1/andrew_stool_out/combined_sequences.fasta'

# Python parameters - select the number of smaller files to create from the main fasta. Equal to queue job limit
bp_python_chunknumber=5000

# Python pbs parameters (to split fasta into chunks). Default should be fine in the known universe.
bp_python_pbs_q='serial'
bp_python_pbs_l='select=1:ncpus=24:mpiprocs=24'

# BLAST parameters  - parameters to search the files by
bp_blast_evalue=200000
bp_blast_matrix='PAM30'
bp_blast_gap_open=9
bp_blast_gap_extend=1
bp_blast_word_size=4
bp_blast_num_threads=24
bp_blast_outfmt=5
bp_blast_BLASTDB='/mnt/lustre/users/mpotgieter1/uniprot/uniprot_current'

# BLAST pbs parameters - Take into account the queue/core limitations and number of chunks chosen for the main fasta
bp_blast_q='smp'  # CHPC normal queue for parallel jobs
bp_blast_l="select=1:ncpus=24:mpiprocs=24" 

# blast_XML_to_csv.py exprort pbs parameters
bp_xml_pbs_q='smp'
bp_xml_pbs_l='select=1:ncpus=24:mpiprocs=24'

# blast summary export table
bp_sum_pbs_q='normal'
bp_sum_pbs_l='select=2:ncpus=24:mpiprocs=24'
bp_sum_aln_cutoff=2
bp_sum_pept2lca=1
